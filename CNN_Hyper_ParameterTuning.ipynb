{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prax0028/Violence_Detection_Model_Codes/blob/main/CNN_Hyper_ParameterTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnDLz9vfzDs7",
        "outputId": "6d0e5807-1980-49e8-f22e-9db6abf1c551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'violence_dataset'...\n",
            "remote: Enumerating objects: 10971, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 10971 (delta 7), reused 0 (delta 0), pack-reused 10956 (from 1)\u001b[K\n",
            "Receiving objects: 100% (10971/10971), 645.81 MiB | 18.71 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n",
            "Updating files: 100% (11065/11065), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Prax0028/violence_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U albumentations==1.2.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkfQyhyZzMfr",
        "outputId": "0e3da05e-542e-4041-a191-aa42ca40802b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albumentations==1.2.1\n",
            "  Downloading albumentations-1.2.1-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.2.1) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from albumentations==1.2.1) (1.13.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.2.1) (0.24.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations==1.2.1) (6.0.2)\n",
            "Collecting qudida>=0.0.4 (from albumentations==1.2.1)\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.2.1) (4.10.0.84)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations==1.2.1) (1.5.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations==1.2.1) (4.12.2)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (3.4.2)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (10.4.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (2024.9.20)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.2.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.2.1) (3.5.0)\n",
            "Downloading albumentations-1.2.1-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.7/116.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Installing collected packages: qudida, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.4.20\n",
            "    Uninstalling albumentations-1.4.20:\n",
            "      Successfully uninstalled albumentations-1.4.20\n",
            "Successfully installed albumentations-1.2.1 qudida-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade opencv-contrib-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aBoioQ7zN21",
        "outputId": "29afdeba-d63e-450b-cbdf-64ab95000ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-contrib-python) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vYT-d-8XzPod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/violence_dataset')"
      ],
      "metadata": {
        "id": "0wwRcpXszPxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms as T\n",
        "\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import utils"
      ],
      "metadata": {
        "id": "ri2TbRJTzRjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from itertools import product\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import albumentations as A\n",
        "\n",
        "# Device setup\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ],
      "metadata": {
        "id": "kKTzlyDHzkzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNNModel, self).__init__()\n",
        "\n",
        "    self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling to reduce spatial dimensions to 1x1\n",
        "        )\n",
        "\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),  # Flatten the feature maps into a 1D vector\n",
        "            nn.Linear(128, 512),  # Assuming input size is (128, 128, 3), output after conv layers will be (128, 16, 16)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 2)  # Output layer with 2 units (for 'violence' and 'non-violence')\n",
        "    )\n",
        "\n",
        "    self.gradient = None\n",
        "\n",
        "\n",
        "  def forward(self,images):\n",
        "\n",
        "    x = self.feature_extractor(images) #activation_maps\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "jrSCmVMMznWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training Function\n",
        "def train_fun(dataloader, model, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for images, labels in tqdm(dataloader):\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        correct_predictions += (predictions == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Evaluation Function\n",
        "def eval_fun(dataloader, model, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    all_actual_labels = []\n",
        "    all_predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader):\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            correct_predictions += (predictions == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "            all_actual_labels.extend(labels.cpu().numpy())\n",
        "            all_predicted_labels.extend(predictions.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    return avg_loss, accuracy, all_actual_labels, all_predicted_labels\n"
      ],
      "metadata": {
        "id": "d2HnXq7xzo5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Data Augmentation\n",
        "train_augs = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.Rotate(),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "valid_augs = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "BuwEC09BzqIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hyperparameter Grid\n",
        "# Batch sizes: Small batch sizes can provide better generalization but may be noisier.\n",
        "batch_sizes = [2, 4, 8, 16, 32]\n",
        "\n",
        "# Learning rates: Typical ranges for CNNs. Experimenting with different magnitudes helps stabilize training.\n",
        "learning_rates = [0.001, 0.0005, 0.0001]\n",
        "\n",
        "# Train-validation splits: Commonly used splits to test model generalization.\n",
        "train_val_splits = [0.7, 0.75, 0.8, 0.85, 0.9]\n",
        "\n",
        "# Optimizers: Different optimizers can affect convergence speed and final performance.\n",
        "#optimizers = ['Adam', 'SGD', 'RMSprop', 'AdamW']\n",
        "optimizers = ['Adam']\n",
        "\n",
        "# Results List\n",
        "results = []\n"
      ],
      "metadata": {
        "id": "qZ6PEmO9zq_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "8ah82rbdrFmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load Data\n",
        "data = pd.read_csv('/content/violence_dataset/image_labels.csv')  # Update this path as needed\n",
        "\n",
        "# Iterate over all combinations of hyperparameters\n",
        "for batch_size, lr, split, optimizer_name in product(batch_sizes, learning_rates, train_val_splits, optimizers):\n",
        "    # Data Split\n",
        "    train_df, valid_df = train_test_split(data, test_size=1-split, random_state=42)\n",
        "    trainset = utils.ImageDataset(train_df, augs=train_augs, data_dir='/content/violence_dataset/')\n",
        "    validset = utils.ImageDataset(valid_df, augs=valid_augs, data_dir='/content/violence_dataset/')\n",
        "\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "    validloader = DataLoader(validset, batch_size=batch_size)\n",
        "\n",
        "    # Initialize model\n",
        "    model = CNNModel().to(DEVICE)\n",
        "\n",
        "    # Set optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr) if optimizer_name == 'Adam' else optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Train for 5 epochs\n",
        "    for epoch in range(5):\n",
        "        train_loss, train_accuracy = train_fun(trainloader, model, optimizer, criterion)\n",
        "        valid_loss, valid_accuracy, _, _ = eval_fun(validloader, model, criterion)\n",
        "\n",
        "        # Only log final accuracy after 20 epochs\n",
        "        if epoch == 4:\n",
        "            results.append({\n",
        "                'batch_size': batch_size,\n",
        "                'learning_rate': lr,\n",
        "                'train_val_split': split,\n",
        "                'optimizer': optimizer_name,\n",
        "                'train_accuracy': train_accuracy,\n",
        "                'valid_accuracy': valid_accuracy\n",
        "            })\n",
        "            # Print final accuracies\n",
        "            print(f\"Final Training Accuracy: {train_accuracy:.4f}, Final Validation Accuracy: {valid_accuracy:.4f}\")\n",
        "\n",
        "        # Save to CSV after each hyperparameter combination finishes\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv('hyperparameter_results.csv', mode='a', index=False, header=not os.path.isfile('hyperparameter_results.csv'))\n",
        "    print(f\"Saved results to CSV for batch_size={batch_size}, lr={lr}, split={split}, optimizer={optimizer_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "MqIs8wbZzsSL",
        "outputId": "1ea4208d-190c-442b-c2a5-f678f7b0c281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 2403/3872 [07:42<04:42,  5.19it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-46f3abb9c791>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Train for 5 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-f12f7e3c9bc6>\u001b[0m in \u001b[0;36mtrain_fun\u001b[0;34m(dataloader, model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}